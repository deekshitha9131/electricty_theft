{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03_modeling.ipynb - DAY 14: Model Comparison\n",
    "\n",
    "**Objective**: Compare all trained models across multiple metrics:\n",
    "- ROC-AUC\n",
    "- PR-AUC  \n",
    "- Top-k anomalies\n",
    "- False positives\n",
    "\n",
    "**Models Evaluated**:\n",
    "1. Isolation Forest (Tuned)\n",
    "2. LSTM Autoencoder\n",
    "3. Deep SVDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, confusion_matrix\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìä DAY 14: COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load All Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_results():\n",
    "    \"\"\"Load results from all three models\"\"\"\n",
    "    base_path = Path(\"../\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # 1. Isolation Forest Results\n",
    "    try:\n",
    "        if_data = pd.read_csv(base_path / \"data/processed/isolation_forest_tuned_results.csv\")\n",
    "        results['isolation_forest'] = {\n",
    "            'scores': if_data['anomaly_score'].values,\n",
    "            'labels': if_data['is_anomaly'].values,\n",
    "            'name': 'Isolation Forest'\n",
    "        }\n",
    "        print(\"‚úÖ Loaded Isolation Forest results\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load Isolation Forest: {e}\")\n",
    "    \n",
    "    # 2. LSTM Autoencoder Results\n",
    "    try:\n",
    "        lstm_dir = base_path / \"data/processed/lstm\"\n",
    "        recon_train = np.load(lstm_dir / \"recon_train.npy\")\n",
    "        recon_val = np.load(lstm_dir / \"recon_val.npy\")\n",
    "        ae_scores = np.concatenate([recon_train, recon_val])\n",
    "        \n",
    "        # Load threshold\n",
    "        with open(base_path / \"models/threshold_config.json\", \"r\") as f:\n",
    "            config = json.load(f)\n",
    "        threshold = config['selected_threshold']\n",
    "        ae_labels = (ae_scores > threshold).astype(int)\n",
    "        \n",
    "        results['autoencoder'] = {\n",
    "            'scores': ae_scores,\n",
    "            'labels': ae_labels,\n",
    "            'threshold': threshold,\n",
    "            'name': 'LSTM Autoencoder'\n",
    "        }\n",
    "        print(\"‚úÖ Loaded Autoencoder results\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load Autoencoder: {e}\")\n",
    "    \n",
    "    # 3. Deep SVDD Results\n",
    "    try:\n",
    "        svdd_scores = np.load(lstm_dir / \"svdd_scores.npy\")\n",
    "        svdd_labels = np.load(lstm_dir / \"svdd_labels.npy\")\n",
    "        \n",
    "        results['svdd'] = {\n",
    "            'scores': svdd_scores,\n",
    "            'labels': svdd_labels,\n",
    "            'name': 'Deep SVDD'\n",
    "        }\n",
    "        print(\"‚úÖ Loaded Deep SVDD results\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load Deep SVDD: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Load all results\n",
    "model_results = load_all_results()\n",
    "print(f\"\\nüìà Loaded {len(model_results)} models for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ROC-AUC Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_roc_metrics(results):\n",
    "    \"\"\"Compute ROC curves and AUC for all models\"\"\"\n",
    "    roc_data = {}\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "    \n",
    "    for i, (model_name, data) in enumerate(results.items()):\n",
    "        scores = data['scores']\n",
    "        labels = data['labels']\n",
    "        \n",
    "        # Convert scores to anomaly scores (higher = more anomalous)\n",
    "        if model_name == 'svdd':\n",
    "            anomaly_scores = -scores  # SVDD: lower = more anomalous\n",
    "        else:\n",
    "            anomaly_scores = scores\n",
    "        \n",
    "        # ROC Curve\n",
    "        fpr, tpr, _ = roc_curve(labels, anomaly_scores)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        \n",
    "        roc_data[model_name] = {\n",
    "            'fpr': fpr,\n",
    "            'tpr': tpr,\n",
    "            'auc': roc_auc\n",
    "        }\n",
    "        \n",
    "        # Plot ROC curve\n",
    "        ax1.plot(fpr, tpr, color=colors[i], lw=2,\n",
    "                label=f'{data[\"name\"]} (AUC = {roc_auc:.3f})')\n",
    "    \n",
    "    # ROC plot formatting\n",
    "    ax1.plot([0, 1], [0, 1], 'k--', lw=1)\n",
    "    ax1.set_xlim([0.0, 1.0])\n",
    "    ax1.set_ylim([0.0, 1.05])\n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.set_title('ROC Curves Comparison')\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # AUC Bar Chart\n",
    "    model_names = [results[k]['name'] for k in roc_data.keys()]\n",
    "    auc_scores = [roc_data[k]['auc'] for k in roc_data.keys()]\n",
    "    \n",
    "    bars = ax2.bar(model_names, auc_scores, color=colors[:len(model_names)], alpha=0.7)\n",
    "    ax2.set_ylabel('ROC-AUC Score')\n",
    "    ax2.set_title('ROC-AUC Comparison')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, score in zip(bars, auc_scores):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../models/trained_models/roc_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return roc_data\n",
    "\n",
    "roc_results = compute_roc_metrics(model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Precision-Recall AUC Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pr_metrics(results):\n",
    "    \"\"\"Compute Precision-Recall curves and AUC\"\"\"\n",
    "    pr_data = {}\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "    \n",
    "    for i, (model_name, data) in enumerate(results.items()):\n",
    "        scores = data['scores']\n",
    "        labels = data['labels']\n",
    "        \n",
    "        # Convert scores to anomaly scores\n",
    "        if model_name == 'svdd':\n",
    "            anomaly_scores = -scores\n",
    "        else:\n",
    "            anomaly_scores = scores\n",
    "        \n",
    "        # PR Curve\n",
    "        precision, recall, _ = precision_recall_curve(labels, anomaly_scores)\n",
    "        pr_auc = auc(recall, precision)\n",
    "        \n",
    "        pr_data[model_name] = {\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'auc': pr_auc\n",
    "        }\n",
    "        \n",
    "        # Plot PR curve\n",
    "        ax1.plot(recall, precision, color=colors[i], lw=2,\n",
    "                label=f'{data[\"name\"]} (AUC = {pr_auc:.3f})')\n",
    "    \n",
    "    # PR plot formatting\n",
    "    ax1.set_xlim([0.0, 1.0])\n",
    "    ax1.set_ylim([0.0, 1.05])\n",
    "    ax1.set_xlabel('Recall')\n",
    "    ax1.set_ylabel('Precision')\n",
    "    ax1.set_title('Precision-Recall Curves')\n",
    "    ax1.legend(loc=\"lower left\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # PR-AUC Bar Chart\n",
    "    model_names = [results[k]['name'] for k in pr_data.keys()]\n",
    "    pr_auc_scores = [pr_data[k]['auc'] for k in pr_data.keys()]\n",
    "    \n",
    "    bars = ax2.bar(model_names, pr_auc_scores, color=colors[:len(model_names)], alpha=0.7)\n",
    "    ax2.set_ylabel('PR-AUC Score')\n",
    "    ax2.set_title('PR-AUC Comparison')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, score in zip(bars, pr_auc_scores):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../models/trained_models/pr_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return pr_data\n",
    "\n",
    "pr_results = compute_pr_metrics(model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Top-K Anomalies Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_top_k_anomalies(results, k_values=[50, 100, 200, 500]):\n",
    "    \"\"\"Analyze precision at top-k anomalies\"\"\"\n",
    "    \n",
    "    topk_data = {}\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "    \n",
    "    for i, (model_name, data) in enumerate(results.items()):\n",
    "        scores = data['scores']\n",
    "        labels = data['labels']\n",
    "        \n",
    "        # Convert to anomaly scores and sort\n",
    "        if model_name == 'svdd':\n",
    "            anomaly_scores = -scores\n",
    "        else:\n",
    "            anomaly_scores = scores\n",
    "        \n",
    "        # Sort by anomaly score (descending)\n",
    "        sorted_indices = np.argsort(anomaly_scores)[::-1]\n",
    "        sorted_labels = labels[sorted_indices]\n",
    "        \n",
    "        # Calculate precision at top-k\n",
    "        precisions_at_k = []\n",
    "        for k in k_values:\n",
    "            if k <= len(sorted_labels):\n",
    "                precision_k = np.mean(sorted_labels[:k])\n",
    "                precisions_at_k.append(precision_k)\n",
    "            else:\n",
    "                precisions_at_k.append(np.nan)\n",
    "        \n",
    "        topk_data[model_name] = {\n",
    "            'k_values': k_values,\n",
    "            'precisions': precisions_at_k\n",
    "        }\n",
    "        \n",
    "        # Plot precision@k curve\n",
    "        ax1.plot(k_values, precisions_at_k, 'o-', color=colors[i], \n",
    "                label=data['name'], linewidth=2, markersize=6)\n",
    "    \n",
    "    ax1.set_xlabel('Top-K Anomalies')\n",
    "    ax1.set_ylabel('Precision@K')\n",
    "    ax1.set_title('Precision at Top-K Anomalies')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(0, 1)\n",
    "    \n",
    "    # Bar chart for precision@100\n",
    "    model_names = [results[k]['name'] for k in topk_data.keys()]\n",
    "    p100_scores = [topk_data[k]['precisions'][1] for k in topk_data.keys()]  # k=100 is index 1\n",
    "    \n",
    "    bars = ax2.bar(model_names, p100_scores, color=colors[:len(model_names)], alpha=0.7)\n",
    "    ax2.set_ylabel('Precision@100')\n",
    "    ax2.set_title('Precision at Top-100 Anomalies')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, score in zip(bars, p100_scores):\n",
    "        if not np.isnan(score):\n",
    "            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                    f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../models/trained_models/topk_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return topk_data\n",
    "\n",
    "topk_results = analyze_top_k_anomalies(model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. False Positive Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_false_positives(results):\n",
    "    \"\"\"Analyze false positive rates and patterns\"\"\"\n",
    "    \n",
    "    fp_data = {}\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "    \n",
    "    model_names = []\n",
    "    fp_rates = []\n",
    "    fn_rates = []\n",
    "    \n",
    "    for i, (model_name, data) in enumerate(results.items()):\n",
    "        labels = data['labels']\n",
    "        predictions = labels  # Using current predictions\n",
    "        \n",
    "        # Confusion matrix\n",
    "        cm = confusion_matrix(labels, predictions)\n",
    "        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, len(labels))\n",
    "        \n",
    "        # Calculate rates\n",
    "        fp_rate = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "        fn_rate = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "        \n",
    "        fp_data[model_name] = {\n",
    "            'fp_rate': fp_rate,\n",
    "            'fn_rate': fn_rate,\n",
    "            'fp_count': fp,\n",
    "            'fn_count': fn,\n",
    "            'confusion_matrix': cm\n",
    "        }\n",
    "        \n",
    "        model_names.append(data['name'])\n",
    "        fp_rates.append(fp_rate)\n",
    "        fn_rates.append(fn_rate)\n",
    "    \n",
    "    # False Positive Rate comparison\n",
    "    bars1 = ax1.bar(model_names, fp_rates, color=colors[:len(model_names)], alpha=0.7)\n",
    "    ax1.set_ylabel('False Positive Rate')\n",
    "    ax1.set_title('False Positive Rate Comparison')\n",
    "    ax1.set_ylim(0, max(fp_rates) * 1.1 if fp_rates else 1)\n",
    "    \n",
    "    for bar, rate in zip(bars1, fp_rates):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(fp_rates)*0.01,\n",
    "                f'{rate:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # False Negative Rate comparison\n",
    "    bars2 = ax2.bar(model_names, fn_rates, color=colors[:len(model_names)], alpha=0.7)\n",
    "    ax2.set_ylabel('False Negative Rate')\n",
    "    ax2.set_title('False Negative Rate Comparison')\n",
    "    ax2.set_ylim(0, max(fn_rates) * 1.1 if fn_rates else 1)\n",
    "    \n",
    "    for bar, rate in zip(bars2, fn_rates):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(fn_rates)*0.01,\n",
    "                f'{rate:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Error rates heatmap\n",
    "    error_matrix = np.array([fp_rates, fn_rates]).T\n",
    "    im = ax3.imshow(error_matrix, cmap='Reds', aspect='auto')\n",
    "    ax3.set_xticks([0, 1])\n",
    "    ax3.set_xticklabels(['False Positive', 'False Negative'])\n",
    "    ax3.set_yticks(range(len(model_names)))\n",
    "    ax3.set_yticklabels(model_names)\n",
    "    ax3.set_title('Error Rates Heatmap')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(len(model_names)):\n",
    "        for j in range(2):\n",
    "            ax3.text(j, i, f'{error_matrix[i, j]:.3f}', \n",
    "                    ha='center', va='center', color='white', fontweight='bold')\n",
    "    \n",
    "    plt.colorbar(im, ax=ax3)\n",
    "    \n",
    "    # Combined error rate (FP + FN)\n",
    "    combined_errors = [fp + fn for fp, fn in zip(fp_rates, fn_rates)]\n",
    "    bars4 = ax4.bar(model_names, combined_errors, color=colors[:len(model_names)], alpha=0.7)\n",
    "    ax4.set_ylabel('Combined Error Rate (FP + FN)')\n",
    "    ax4.set_title('Total Error Rate Comparison')\n",
    "    \n",
    "    for bar, rate in zip(bars4, combined_errors):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(combined_errors)*0.01,\n",
    "                f'{rate:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../models/trained_models/false_positive_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return fp_data\n",
    "\n",
    "fp_results = analyze_false_positives(model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comprehensive Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_summary(roc_results, pr_results, topk_results, fp_results, model_results):\n",
    "    \"\"\"Create comprehensive comparison table and final recommendation\"\"\"\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    \n",
    "    for model_name, data in model_results.items():\n",
    "        model_display_name = data['name']\n",
    "        \n",
    "        # Get metrics\n",
    "        roc_auc = roc_results[model_name]['auc']\n",
    "        pr_auc = pr_results[model_name]['auc']\n",
    "        precision_100 = topk_results[model_name]['precisions'][1]  # Top-100\n",
    "        fp_rate = fp_results[model_name]['fp_rate']\n",
    "        fn_rate = fp_results[model_name]['fn_rate']\n",
    "        \n",
    "        # Anomaly detection rate\n",
    "        anomaly_rate = np.mean(data['labels'])\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Model': model_display_name,\n",
    "            'ROC-AUC': roc_auc,\n",
    "            'PR-AUC': pr_auc,\n",
    "            'Precision@100': precision_100,\n",
    "            'False Positive Rate': fp_rate,\n",
    "            'False Negative Rate': fn_rate,\n",
    "            'Anomaly Rate': anomaly_rate\n",
    "        })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Display summary table\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üèÜ COMPREHENSIVE MODEL COMPARISON SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(summary_df.to_string(index=False, float_format='%.3f'))\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Find best models for each metric\n",
    "    best_roc = summary_df.loc[summary_df['ROC-AUC'].idxmax(), 'Model']\n",
    "    best_pr = summary_df.loc[summary_df['PR-AUC'].idxmax(), 'Model']\n",
    "    best_p100 = summary_df.loc[summary_df['Precision@100'].idxmax(), 'Model']\n",
    "    lowest_fp = summary_df.loc[summary_df['False Positive Rate'].idxmin(), 'Model']\n",
    "    \n",
    "    print(f\"\\nüéØ BEST PERFORMERS:\")\n",
    "    print(f\"   ‚Ä¢ Best ROC-AUC: {best_roc} ({summary_df[summary_df['Model']==best_roc]['ROC-AUC'].iloc[0]:.3f})\")\n",
    "    print(f\"   ‚Ä¢ Best PR-AUC: {best_pr} ({summary_df[summary_df['Model']==best_pr]['PR-AUC'].iloc[0]:.3f})\")\n",
    "    print(f\"   ‚Ä¢ Best Precision@100: {best_p100} ({summary_df[summary_df['Model']==best_p100]['Precision@100'].iloc[0]:.3f})\")\n",
    "    print(f\"   ‚Ä¢ Lowest False Positives: {lowest_fp} ({summary_df[summary_df['Model']==lowest_fp]['False Positive Rate'].iloc[0]:.3f})\")\n",
    "    \n",
    "    # Overall recommendation based on weighted score\n",
    "    summary_df['Overall_Score'] = (\n",
    "        0.3 * summary_df['ROC-AUC'] + \n",
    "        0.3 * summary_df['PR-AUC'] + \n",
    "        0.2 * summary_df['Precision@100'] + \n",
    "        0.2 * (1 - summary_df['False Positive Rate'])  # Lower FP is better\n",
    "    )\n",
    "    \n",
    "    best_overall = summary_df.loc[summary_df['Overall_Score'].idxmax(), 'Model']\n",
    "    best_score = summary_df['Overall_Score'].max()\n",
    "    \n",
    "    print(f\"\\nüèÖ FINAL RECOMMENDATION: {best_overall}\")\n",
    "    print(f\"   Overall Score: {best_score:.3f}/1.000\")\n",
    "    print(f\"   (Weighted: 30% ROC-AUC + 30% PR-AUC + 20% P@100 + 20% Low-FP)\")\n",
    "    \n",
    "    # Save summary\n",
    "    summary_df.to_csv('../models/comprehensive_model_comparison.csv', index=False)\n",
    "    print(f\"\\nüíæ Saved: comprehensive_model_comparison.csv\")\n",
    "    \n",
    "    return summary_df, best_overall\n",
    "\n",
    "final_summary, recommended_model = create_comprehensive_summary(\n",
    "    roc_results, pr_results, topk_results, fp_results, model_results\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Visualization Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_final_dashboard(final_summary):\n",
    "    \"\"\"Create a comprehensive dashboard visualization\"\"\"\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 12))\n",
    "    gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. ROC-AUC comparison\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    bars1 = ax1.bar(final_summary['Model'], final_summary['ROC-AUC'], \n",
    "                   color=['#1f77b4', '#ff7f0e', '#2ca02c'][:len(final_summary)], alpha=0.8)\n",
    "    ax1.set_title('ROC-AUC Scores', fontweight='bold')\n",
    "    ax1.set_ylim(0, 1)\n",
    "    for bar, score in zip(bars1, final_summary['ROC-AUC']):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. PR-AUC comparison\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    bars2 = ax2.bar(final_summary['Model'], final_summary['PR-AUC'], \n",
    "                   color=['#1f77b4', '#ff7f0e', '#2ca02c'][:len(final_summary)], alpha=0.8)\n",
    "    ax2.set_title('PR-AUC Scores', fontweight='bold')\n",
    "    ax2.set_ylim(0, 1)\n",
    "    for bar, score in zip(bars2, final_summary['PR-AUC']):\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. Precision@100\n",
    "    ax3 = fig.add_subplot(gs[0, 2])\n",
    "    bars3 = ax3.bar(final_summary['Model'], final_summary['Precision@100'], \n",
    "                   color=['#1f77b4', '#ff7f0e', '#2ca02c'][:len(final_summary)], alpha=0.8)\n",
    "    ax3.set_title('Precision@100', fontweight='bold')\n",
    "    ax3.set_ylim(0, 1)\n",
    "    for bar, score in zip(bars3, final_summary['Precision@100']):\n",
    "        if not np.isnan(score):\n",
    "            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "                    f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. False Positive Rate\n",
    "    ax4 = fig.add_subplot(gs[0, 3])\n",
    "    bars4 = ax4.bar(final_summary['Model'], final_summary['False Positive Rate'], \n",
    "                   color=['#d62728', '#ff7f0e', '#2ca02c'][:len(final_summary)], alpha=0.8)\n",
    "    ax4.set_title('False Positive Rate', fontweight='bold')\n",
    "    for bar, score in zip(bars4, final_summary['False Positive Rate']):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(final_summary['False Positive Rate'])*0.05,\n",
    "                f'{score:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 5. Radar chart for overall comparison\n",
    "    ax5 = fig.add_subplot(gs[1, :2], projection='polar')\n",
    "    \n",
    "    metrics = ['ROC-AUC', 'PR-AUC', 'Precision@100', 'Low FP Rate']\n",
    "    angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "    \n",
    "    colors_radar = ['#1f77b4', '#ff7f0e', '#2ca02c']\n",
    "    \n",
    "    for i, (_, row) in enumerate(final_summary.iterrows()):\n",
    "        values = [\n",
    "            row['ROC-AUC'],\n",
    "            row['PR-AUC'], \n",
    "            row['Precision@100'] if not np.isnan(row['Precision@100']) else 0,\n",
    "            1 - row['False Positive Rate']  # Convert to \"Low FP Rate\"\n",
    "        ]\n",
    "        values += values[:1]\n",
    "        \n",
    "        ax5.plot(angles, values, 'o-', linewidth=2, label=row['Model'], \n",
    "                color=colors_radar[i], markersize=6)\n",
    "        ax5.fill(angles, values, alpha=0.1, color=colors_radar[i])\n",
    "    \n",
    "    ax5.set_xticks(angles[:-1])\n",
    "    ax5.set_xticklabels(metrics)\n",
    "    ax5.set_ylim(0, 1)\n",
    "    ax5.set_title('Overall Performance Radar', fontweight='bold', pad=20)\n",
    "    ax5.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n",
    "    \n",
    "    # 6. Summary table\n",
    "    ax6 = fig.add_subplot(gs[1, 2:])\n",
    "    ax6.axis('tight')\n",
    "    ax6.axis('off')\n",
    "    \n",
    "    table_data = final_summary[['Model', 'ROC-AUC', 'PR-AUC', 'Precision@100', \n",
    "                               'False Positive Rate', 'Anomaly Rate']].round(3)\n",
    "    \n",
    "    table = ax6.table(cellText=table_data.values, colLabels=table_data.columns,\n",
    "                     cellLoc='center', loc='center', bbox=[0, 0, 1, 1])\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1, 2)\n",
    "    \n",
    "    # Style the table\n",
    "    for i in range(len(table_data.columns)):\n",
    "        table[(0, i)].set_facecolor('#4CAF50')\n",
    "        table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "    \n",
    "    ax6.set_title('Performance Summary Table', fontweight='bold', pad=20)\n",
    "    \n",
    "    # 7. Recommendation box\n",
    "    ax7 = fig.add_subplot(gs[2, :])\n",
    "    ax7.axis('off')\n",
    "    \n",
    "    recommendation_text = f\"\"\"\n",
    "üèÜ FINAL RECOMMENDATION: {recommended_model}\n",
    "\n",
    "Based on comprehensive evaluation across multiple metrics:\n",
    "‚Ä¢ ROC-AUC: Measures overall discrimination ability\n",
    "‚Ä¢ PR-AUC: Focuses on precision-recall trade-off (important for imbalanced data)\n",
    "‚Ä¢ Precision@100: Practical metric for investigating top anomalies\n",
    "‚Ä¢ False Positive Rate: Critical for operational deployment\n",
    "\n",
    "The {recommended_model} demonstrates the best balance across all evaluation criteria.\n",
    "    \"\"\"\n",
    "    \n",
    "    ax7.text(0.5, 0.5, recommendation_text, transform=ax7.transAxes, \n",
    "            fontsize=12, ha='center', va='center',\n",
    "            bbox=dict(boxstyle='round,pad=1', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    plt.suptitle('DAY 14: COMPREHENSIVE MODEL COMPARISON DASHBOARD', \n",
    "                fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    plt.savefig('../models/trained_models/final_comparison_dashboard.png', \n",
    "               dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "create_final_dashboard(final_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Day 14 Completion Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ DAY 14 COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "deliverables = [\n",
    "    \"üìä ROC-AUC comparison across all models\",\n",
    "    \"üìà PR-AUC analysis for imbalanced data\", \n",
    "    \"üéØ Top-K anomalies precision evaluation\",\n",
    "    \"‚ùå False positive rate analysis\",\n",
    "    \"üìã Comprehensive comparison table\",\n",
    "    \"üèÜ Final model recommendation\",\n",
    "    \"üì± Interactive comparison dashboard\",\n",
    "    \"üíæ All results saved to /models/\"\n",
    "]\n",
    "\n",
    "print(\"\\nüéØ DELIVERABLES COMPLETED:\")\n",
    "for deliverable in deliverables:\n",
    "    print(f\"   {deliverable}\")\n",
    "\n",
    "print(f\"\\nüèÖ RECOMMENDED MODEL: {recommended_model}\")\n",
    "print(\"\\nüìÅ FILES GENERATED:\")\n",
    "generated_files = [\n",
    "    \"roc_comparison.png\",\n",
    "    \"pr_comparison.png\", \n",
    "    \"topk_comparison.png\",\n",
    "    \"false_positive_analysis.png\",\n",
    "    \"final_comparison_dashboard.png\",\n",
    "    \"comprehensive_model_comparison.csv\"\n",
    "]\n",
    "\n",
    "for file in generated_files:\n",
    "    print(f\"   ‚úÖ {file}\")\n",
    "\n",
    "print(\"\\nüöÄ READY FOR DAY 15: Streamlit Dashboard Development!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}